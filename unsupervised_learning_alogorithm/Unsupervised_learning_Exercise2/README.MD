
ğŸ· Wine Clustering Project â€” Codebasics ML Practice

ğŸ“˜ Overview

This project applies Unsupervised Learning (Clustering) techniques to group wines based on their chemical properties.
It was completed as part of the Codebasics Machine Learning Course.

Using K-Means, Hierarchical Clustering, and DBSCAN, we aim to:
	â€¢	Discover natural clusters among different wine types.
	â€¢	Compare performance using metrics like SSE (Sum of Squared Errors) and Silhouette Score.
	â€¢	Visualize clusters using PCA (Principal Component Analysis) for dimensionality reduction.

â¸»

ğŸ“‚ Project Structure

wine-clustering/
â”‚
â”œâ”€â”€ assignment_solution.ipynb      # Main Jupyter Notebook (solution)
â”œâ”€â”€ assignment.ipynb               # Starter/assignment notebook
â”œâ”€â”€ wine_clustering_data.csv       # Dataset used in project
â”œâ”€â”€ wine_clustering_results.csv    # Final dataset with cluster labels (generated)
â”œâ”€â”€ kmeans_centers_original_scale.csv  # KMeans cluster centroids (optional output)
â”œâ”€â”€ README.md                      # Project documentation (this file)
â””â”€â”€ images/                        # Folder for plots/screenshots (optional)


â¸»

ğŸ§  Key Learning Concepts
	â€¢	Feature Scaling (Standardization)
	â€¢	K-Means Clustering
	â€¢	Elbow Method for optimal K
	â€¢	Silhouette Score for cluster validation
	â€¢	Hierarchical Clustering (Ward linkage)
	â€¢	DBSCAN (Density-Based Clustering)
	â€¢	Dimensionality Reduction using PCA
	â€¢	Cluster Visualization and Interpretation

â¸»

âš™ï¸ Tech Stack
	â€¢	Language: Python 3.x
	â€¢	Libraries:
pandas, numpy, matplotlib, seaborn,
scikit-learn, scipy

â¸»

ğŸš€ Workflow Summary
	1.	Load Data â†’ wine_clustering_data.csv
Checked shape, missing values, and descriptive statistics.
	2.	Exploratory Data Analysis (EDA)
	â€¢	Histograms to understand feature distributions
	â€¢	Correlation heatmap
	3.	Feature Scaling
Standardized data using StandardScaler to ensure equal weight for all features.
	4.	K-Means Clustering
	â€¢	Applied the Elbow Method to identify optimal K.
	â€¢	Evaluated clusters using Silhouette Score.
	â€¢	Visualized clusters with PCA (2D plot).
	5.	Hierarchical Clustering
	â€¢	Generated Dendrogram using Wardâ€™s linkage.
	â€¢	Compared with K-Means cluster patterns.
	6.	DBSCAN (optional)
	â€¢	Tested density-based clustering for anomaly detection or non-spherical clusters.
	7.	Results & Interpretation
	â€¢	KMeans and Hierarchical both found ~3 clusters.
	â€¢	Visuals showed clear group separation.

â¸»

ğŸ“Š Results

Algorithm	Clusters (K)	Silhouette Score	Notes
K-Means	3	~0.26â€“0.35	Balanced cluster separation
Agglomerative (Ward)	3	~0.25â€“0.33	Similar pattern as K-Means
DBSCAN	Varies	Depends on eps	Detects outliers, non-spherical clusters


â¸»

ğŸ“ˆ Visualizations

Add these screenshots to an /images folder and reference them here:
	1.	Elbow Method Plot
	2.	PCA Cluster Visualization (K-Means)
	3.	Dendrogram (Hierarchical Clustering)
	4.	Agglomerative Cluster Plot

Example markdown embed:

![Elbow Plot](images/elbow_plot.png)
![PCA Clusters](images/pca_clusters.png)
![Dendrogram](images/dendrogram.png)


â¸»

ğŸ’¾ Outputs
	â€¢	wine_clustering_results.csv â†’ Dataset with cluster labels (kmeans_label, agg_label, dbscan_label)
	â€¢	kmeans_centers_original_scale.csv â†’ Cluster centroids in original feature scale

â¸»

ğŸ§© How to Run Locally
	1.	Clone this repository:

git clone https://github.com/<your-username>/wine-clustering.git
cd wine-clustering


	2.	Install dependencies:

pip install -r requirements.txt


	3.	Launch Jupyter:

jupyter notebook


	4.	Open assignment_solution.ipynb and run all cells.

â¸»

ğŸ§¾ Requirements.txt

pandas
numpy
matplotlib
seaborn
scikit-learn
scipy


â¸»

ğŸ§  Insights
	â€¢	Clustering can reveal structure in unlabeled data.
	â€¢	Scaling is crucial â€” without it, distance-based methods fail.
	â€¢	KMeans and Hierarchical produced consistent grouping â†’ data has well-separated clusters.
	â€¢	DBSCAN helps detect noise or outliers.

â¸»

ğŸ§‘â€ğŸ’» Author

Vishal
AI/ML Engineer | 


