# ğŸ¨ Hotel Reservations Classification â€” Machine Learning Project

This project is part of the **Codebasics Machine Learning Course** and focuses on predicting **whether a hotel reservation will be canceled or not**.  
The dataset contains detailed booking information such as the number of adults, children, booking lead time, price per room, and more.

---

## ğŸ“‚ Dataset
**Dataset Name:** `hotel_reservations.csv`  
**Source:** [Kaggle - Hotel Reservations Classification Dataset](https://www.kaggle.com/datasets/ahsan81/hotel-reservations-classification-dataset)

### Features
- `no_of_adults`: Number of adults  
- `no_of_children`: Number of children  
- `lead_time`: Days between booking and arrival  
- `avg_price_per_room`: Average price per day  
- `booking_status`: Target variable (Canceled / Not Canceled)  
â€¦and more booking-related details like meal plan, room type, etc.

---

## ğŸ§¹ Task 1: Data Preprocessing and EDA
- Imported the dataset and performed an overview check (`df.head()`, `df.shape`)
- Dropped irrelevant columns (`booking_id`, `arrival_year`, `arrival_month`, `arrival_date`)
- Visualized data:
  - **Booking status countplot**
  - **Boxplot for lead time and price per room**
- Encoded categorical variables using **One-Hot Encoding**

---

## ğŸ“ Task 2: Feature Scaling
- Scaled numerical features (`lead_time`, `avg_price_per_room`) using `StandardScaler`
- Defined:
  - **X** = Features
  - **y** = Target variable (`booking_status`)

---

## ğŸ” Task 3: Implementing K-Fold Cross Validation
- Used **KFold(n_splits=7, shuffle=True)**  
- Compared models:
  - **Naive Bayes**
  - **Logistic Regression**
  - **Decision Tree**
- Evaluated each modelâ€™s **average cross-validation accuracy**

---

## âš™ï¸ Task 4: Stratified K-Fold Cross Validation
- Ensured class balance using `StratifiedKFold`
- Compared results to regular K-Fold for consistency

---

## ğŸ” Task 5: Grid Search CV
- Applied **GridSearchCV** to tune:
  - **Naive Bayes:** `var_smoothing` = [1e-9, 1e-8, 1e-7]
  - **Random Forest:** `n_estimators` = [10, 50, 100], `max_depth` = [5, 10, 15]
- Extracted **best parameters** for each model

---

## ğŸ² Task 6: Randomized Search CV
- Used **RandomizedSearchCV** for `RandomForestClassifier`
- Parameters:
  - `n_estimators`: [10, 50, 100]
  - `max_depth`: [5, 10, 15, 20]
  - `cv = 7`, `n_iter = 3`
- Printed **best parameters** and performance summary

---

## ğŸ“Š Results Summary
| Model | Method | Accuracy (approx) |
|-------|---------|------------------|
| Naive Bayes | K-Fold CV | 0.40 |
| Logistic Regression | K-Fold CV | 0.80 |
| Decision Tree | K-Fold CV | 0.85 |
| Random Forest (Tuned) | Randomized Search CV | 0.88 |

---

## ğŸ§© Key Learnings
- Practiced **cross-validation** and **model selection techniques**
- Understood **hyperparameter tuning** using GridSearchCV & RandomizedSearchCV
- Observed how **Random Forest** performance improves with tuning
- Reinforced workflow: *EDA â†’ Feature Scaling â†’ Model Evaluation â†’ Hyperparameter Optimization*

---

## ğŸ§‘â€ğŸ’» Tech Stack
- **Python**
- **Scikit-learn**
- **Matplotlib**, **Seaborn**
- **Pandas**, **NumPy**
- **XGBoost**

---

## ğŸ“˜ Author
**Vishal** â€” AI/ML Engineer
Real-world ML projects as part of the Codebasics ML course.
